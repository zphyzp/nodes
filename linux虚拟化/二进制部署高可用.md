# 二进制部署k8s 1.19

## master节点：

**`注意：节点1生成的证书，服务文件，及配置文件均需要拷贝至个节点，并赋予相应权限`**

​	**`证书文件均需赋予相应权限`**

### 1,创建ca根证书

```bash
执行, 并将生成的文件拷贝至各节点的/etc/kubernetes/pki目录下
openssl genrsa -out ca.key 2048
openssl req -x509 -new -nodes -key ca.key -subj "/CN=10.254.253.121" -days 36500 -out ca.crt         
```

### 2,etcd

```bash
github下载二进制包并解压：etcd-v3.4.13-linux-amd64.tar.gz
​拷贝etcd和etcdctl文件至/usr/bin目录下
​创建systemd服务：
	chmod +x /usr/lib/systemd/system/etcd.service
	vim /usr/lib/systemd/system/etcd.service

#etcd.service
[Unit]
Description=etcd key-value store
Documentation=https://github.com/etcd-io/etcd
After=network.target

[Service]
EnvironmentFile=/etc/etcd/etcd.conf
ExecStart=/usr/bin/etcd
Restart=always

[Install]
WantedBy=multi-user.target
```
```bash
创建etcd CA证书：
    mkdir /etc/etcd/pki目录下
    vim etcd_ssl.cnf:
    
#etcd_ssl.cnf
[ req ]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[ req_distinguished_name ]

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[ alt_names ]
IP.1 = 192.168.160.11
IP.2 = 192.168.160.12
IP.3 = 192.168.160.13

    
​生成服务端与客户端证书,并将生成的文件拷贝至各节点的/etc/etcd/pki目录下
    服务端：
   
    openssl genrsa -out etcd_server.key 2048
    openssl req -new -key etcd_server.key -config etcd_ssl.cnf -subj "/CN=etcd-server" -out etcd_server.csr
    openssl x509 -req -in etcd_server.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -days 36500 -extensions v3_req -extfile etcd_ssl.cnf -out etcd_server.crt

    客户端：
    openssl genrsa -out etcd_client.key 2048
    openssl req -new -key etcd_client.key -config etcd_ssl.cnf -subj "/CN=etcd-client" -out etcd_client.csr
    openssl x509 -req -in etcd_client.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -days 36500 -extensions v3_req -extfile etcd_ssl.cnf -out etcd_client.crt

​etcd配置文件

vim /etc/etcd/etcd.conf
    
#etcd.conf
ETCD_NAME=etcd1
ETCD_DATA_DIR=/etc/etcd/data

ETCD_CERT_FILE=/etc/etcd/pki/etcd_server.crt
ETCD_KEY_FILE=/etc/etcd/pki/etcd_server.key
ETCD_TRUSTED_CA_FILE=/etc/kubernetes/pki/ca.crt
ETCD_CLIENT_CERT_AUTH=true
ETCD_LISTEN_CLIENT_URLS=https://192.168.160.11:2379
ETCD_ADVERTISE_CLIENT_URLS=https://192.168.160.11:2379

ETCD_PEER_CERT_FILE=/etc/etcd/pki/etcd_server.crt
ETCD_PEER_KEY_FILE=/etc/etcd/pki/etcd_server.key
ETCD_PEER_TRUSTED_CA_FILE=/etc/kubernetes/pki/ca.crt
ETCD_LISTEN_PEER_URLS=https://192.168.160.11:2380
ETCD_INITIAL_ADVERTISE_PEER_URLS=https://192.168.160.11:2380

ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster
ETCD_INITIAL_CLUSTER="etcd1=https://192.168.160.11:2380,etcd2=https://192.168.160.12:2380,etcd3=https://192.168.160.13:2380"
ETCD_INITIAL_CLUSTER_STATE=new

​启动集群：
    systemctl restart etcd && systemctl enable etcd

​验证
    etcdctl --cacert=/etc/kubernetes/pki/ca.crt --cert=/etc/etcd/pki/etcd_client.crt --key=/etc/etcd/pki/etcd_client.key --endpoints=https://10.254.253.121:2379,https://10.254.253.131:2379,https://10.254.253.141:2379 endpoint health
```
### 3,配置服务端apiserver

```bash

​kubernetes github下载二进制包并解压：kubernetes-server-linux-amd64.tar.gz,将server/bin下的文件拷贝值/usr/bin目录下并赋予可执行权限

​配置ca证书,生成证书并拷贝到/etc/kubernetes/pki目录下并赋予权限
    vim master_ssl.cnf
    
# master_ssl.cnf

[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
DNS.5 = k8s-1
DNS.6 = k8s-2
DNS.7 = k8s-3
IP.1 = 169.169.0.1
IP.2 = 192.168.18.3
IP.3 = 192.168.18.4
IP.4 = 192.168.18.5
IP.5 = 192.168.18.100

​创建服务端CA证书
openssl genrsa -out apiserver.key 2048
openssl req -new -key apiserver.key -config master_ssl.cnf -subj "/CN=10.254.253.121" -out apiserver.csr
openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 36500 -extensions v3_req -extfile master_ssl.cnf -out apiserver.crt

​为apiserver创建系统服务,拷贝至各节点,并赋予执行权限
vim /usr/lib/systemd/system/kube-apiserver.service
#kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/etc/kubernetes/apiserver
ExecStart=/usr/bin/kube-apiserver $KUBE_API_ARGS
Restart=always

[Install]
WantedBy=multi-user.target

​创建apiserver配置文件，拷贝至各节点
vim /etc/kubernetes/apiserver

#apiserver
KUBE_API_ARGS="--insecure-port=0 \
--secure-port=6443 \
--tls-cert-file=/etc/kubernetes/pki/apiserver.crt \
--tls-private-key-file=/etc/kubernetes/pki/apiserver.key \
--client-ca-file=/etc/kubernetes/pki/ca.crt \
--apiserver-count=3 --endpoint-reconciler-type=master-count \
--etcd-servers=https://192.168.18.3:2379,https://192.168.18.4:2379,https://192.168.18.5:2379 \
--etcd-cafile=/etc/kubernetes/pki/ca.crt \
--etcd-certfile=/etc/etcd/pki/etcd_client.crt \
--etcd-keyfile=/etc/etcd/pki/etcd_client.key \
--service-cluster-ip-range=169.169.0.0/16 \
--service-node-port-range=30000-32767 \
--allow-privileged=true \
--logtostderr=false --log-dir=/var/log/kubernetes --v=0"

​启动并开机启动apiserver
systemctl start kube-apiserver && systemctl enable kube-apiserver
```

### 4,配置客户端

```bash
创建客户端CA证书，拷贝值各节点

openssl genrsa -out client.key 2048
openssl req -new -key client.key -subj "/CN=admin" -out client.csr
openssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 36500

​创建建客户端连接apiserver的配置文件,拷贝至各节点

vim /etc/kubernetes/kubeconfig

#kubeconfig
apiVersion: v1
kind: Config
clusters:
- name: default
  cluster:
    server: https://192.168.18.100:9443
    certificate-authority: /etc/kubernetes/pki/ca.crt
users:
- name: admin
  user:
    client-certificate: /etc/kubernetes/pki/client.crt
    client-key: /etc/kubernetes/pki/client.key
contexts:
- context:
    cluster: default
    user: admin
  name: default
current-context: default

​部署controller-manager,拷贝至各节点
    部署服务：
        vim /usr/lib/systemd/system/kube-controller-manager.service
        
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/etc/kubernetes/controller-manager
ExecStart=/usr/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_ARGS
Restart=always

[Install]
WantedBy=multi-user.target

    部署配置文件
        vim /etc/kubernetes/controller-manager
        
KUBE_CONTROLLER_MANAGER_ARGS="--kubeconfig=/etc/kubernetes/kubeconfig \
--leader-elect=true \
--service-cluster-ip-range=169.169.0.0/16 \                #节点IP范围，需与dns同网段
--service-account-private-key-file=/etc/kubernetes/pki/apiserver.key \
--root-ca-file=/etc/kubernetes/pki/ca.crt \
--log-dir=/var/log/kubernetes --logtostderr=false --v=0 \
--allocate-node-cidrs=true \
--cluster-cidr=169.168.0.0/16 "        #容器IP范围，需与flannel中network字段内容同网段
        
    启动服务：
        systemctl start kube-controller-manager && systemctl enable kube-controller-manager

​部署controller-scheduler,拷贝至各节点
    部署服务：
        vim /usr/lib/systemd/system/kube-scheduler.service
        
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/etc/kubernetes/scheduler
ExecStart=/usr/bin/kube-scheduler $KUBE_SCHEDULER_ARGS
Restart=always
        
    部署配置文件
        vim /etc/kubernetes/scheduler
        
KUBE_SCHEDULER_ARGS="--kubeconfig=/etc/kubernetes/kubeconfig \
--leader-elect=true \
--logtostderr=false --log-dir=/var/log/kubernetes --v=0"
        
    启动服务：
        systemctl start kube-scheduler && systemctl enable kube-scheduler
```
### 5,harproxy和keepalived

```bash
部署haproxy
    节点1、2部署haproxy:
        yum 安装haproxy
        修改haproxy配置文件
        
#haproxy.cfg
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4096
    user        haproxy
    group       haproxy
    daemon
    stats socket /var/lib/haproxy/stats

defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option                  http-server-close
    option                  forwardfor    except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

frontend  kube-apiserver
    mode                 tcp
    bind                 *:9443
    option               tcplog
    default_backend      kube-apiserver

listen stats
    mode                 http
    bind                 *:8888
    stats auth           admin:password
    stats refresh        5s
    stats realm          HAProxy\ Statistics
    stats uri            /stats
    log                  127.0.0.1 local3 err

backend kube-apiserver
    mode        tcp
    balance     roundrobin
    server  k8s-master1 192.168.18.3:6443 check
    server  k8s-master2 192.168.18.4:6443 check
    server  k8s-master3 192.168.18.5:6443 check
    
若haproxy无法正常启动，报无法bind socket，尝试如下两种方法
1， setsebool -P haproxy_connect_any=1 执行后重启haproxy 
2，vi   /etc/sysctl.conf
   添加：net.ipv4.ip_nonlocal_bind=1
   sysctl -p
   重启HAproxy

        
    登录haproxy web验证节点状态
        http://192.168.160.11:8888/stats

​部署keepalived
	yum安装

	编写检查脚本拷贝至/usr/bin目录下
# check-haproxy.sh
#!/bin/bash

count=`netstat -apn | grep 9443 | wc -l`

if [ $count -gt 0 ]; then
    exit 0
else
    exit 1
fi

	修改配置文件
	
# keepalived.conf - master 1
! Configuration File for keepalived

global_defs {
   router_id LVS_1
}

vrrp_script checkhaproxy
{
    script "/usr/bin/check-haproxy.sh"
    interval 2
    weight -30
}

vrrp_instance VI_1 {
    state MASTER
    interface ens33
    virtual_router_id 51
    priority 100
    advert_int 1

    virtual_ipaddress {
        192.168.18.100/24 dev ens33
    }

    authentication {
        auth_type PASS
        auth_pass password
    }

    track_script {
        checkhaproxy
    }
}
        
# keepalived.conf - master 2
! Configuration File for keepalived

global_defs {
   router_id LVS_2
}

vrrp_script checkhaproxy
{
    script "/usr/bin/check-haproxy.sh"
    interval 2
    weight -30
}

vrrp_instance VI_1 {

    state BACKUP
    interface ens33
    virtual_router_id 51
    priority 100
    advert_int 1

    virtual_ipaddress {
        192.168.18.100/24 dev ens33
    }

    authentication {
        auth_type PASS
        auth_pass password
    }

    track_script {
        checkhaproxy
    }
}

```

### 6，验证组件

```bash
[root@localhost pki]# kubectl --kubeconfig=/etc/kubernetes/kubeconfig get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok                  
scheduler            Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
etcd-2               Healthy   {"health":"true"}   
etcd-1               Healthy   {"health":"true"}  
```

## node节点

### 1、初始化环境docker,swapoff,防火墙，iptables等

### 2、拷贝master节点的证书及kubeconfig至node节点的/etc/kubernetes/目录下

### 3、部署kubelet

```bash
#copy kubelet和kuebe-proxy至node节点的/usr/bin下

#  vim /usr/lib/systemd/system/kubelet.service

[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/kubernetes/kubernetes
After=docker.target

[Service]
EnvironmentFile=/etc/kubernetes/kubelet
ExecStart=/usr/bin/kubelet $KUBELET_ARGS
Restart=always

[Install]
WantedBy=multi-user.target


#  vim /etc/kubernetes/kubelet

KUBELET_ARGS="--kubeconfig=/etc/kubernetes/kubeconfig --config=/etc/kubernetes/kubelet.config \
--hostname-override=k8s-node01 \
--network-plugin=cni \
--logtostderr=false --log-dir=/var/log/kubernetes --v=0"


#  vim /etc/kubernetes/kubelet.config
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
cgroupDriver: systemd
clusterDNS: ["169.169.0.100"]	                #与coredns的ip相同			
clusterDomain: cluster.local
authentication:
  anonymous:
    enabled: true
    
#启动服务
systemctl start kubelet && systemctl enable kubelet
```

### 4、部署kube-proxy

```bash
# vim /usr/lib/systemd/system/kube-proxy.service

[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
EnvironmentFile=/etc/kubernetes/proxy
ExecStart=/usr/bin/kube-proxy $KUBE_PROXY_ARGS
Restart=always

[Install]
WantedBy=multi-user.target



# vim /etc/kubernetes/proxy

KUBE_PROXY_ARGS="--kubeconfig=/etc/kubernetes/kubeconfig \
--hostname-override=k8s-node01 \
--proxy-mode=iptables \
--logtostderr=false --log-dir=/var/log/kubernetes --v=0"

#启动服务
systemctl start kube-proxy && systemctl enable kube-proxy
```

### 5、验证节点

```bash
[root@localhost pki]# kubectl --kubeconfig=/etc/kubernetes/kubeconfig get nodes
NAME         STATUS     ROLES    AGE     VERSION
k8s-node01   NotReady   <none>   2m32s   v1.19.0
```

### 6、部署cni网络插件
先下载flannel和pause镜像并tag相应镜像名
```bash
# 下载地址：
https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v0.8.6.tgz

# 解压二进制包并移动到默认工作目录：
mkdir /opt/cni/bin
tar zxvf cni-plugins-linux-amd64-v0.8.6.tgz -C /opt/cni/bin

# 部署CNI网络(配置好里面的pause和flannel镜像)：
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
sed -i -r "s#quay.io/coreos/flannel:.*-amd64#lizhenliang/flannel:v0.12.0-amd64#g" kube-flannel.yml
```

### 7、部署coredns

1、下载coredns 1.7.0 镜像

2、下载coredns配置为文件（文件参考内容如下）

3、apply coredns

```yml
#coredns配置文件
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
  labels:
      addonmanager.kubernetes.io/mode: EnsureExists
data:
  Corefile: |
    .:53 {
        errors
        health {
            lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {      #cluster.local与kubelet的clusterDomain的值相对应
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
            ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
            max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: k8s-app
                    operator: In
                    values: ["kube-dns"]
              topologyKey: kubernetes.io/hostname
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeSelector:
        kubernetes.io/os: linux
      containers:
      - name: coredns
        image: hub.tjptvm.com/k8s/coredns:1.7.0   #根据实际情况填写
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 200Mi						#根据实际情况填写
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 169.169.0.100		#与kubelet配置文件的dns地址相同
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP
```

#### 验证dns

```yml
#部署一个dig
apiVersion: v1
kind: Pod
metadata:
  name: dig
  namespace: default
spec:
  containers:
  - name: dig
    image:  azukiapp/dig
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
  
#[root@localhost ~]# kubectl --kubeconfig=/etc/kubernetes/kubeconfig exec -ti dig -- nslookup kubernetes
Server:		169.169.0.100
Address:	169.169.0.100#53

Name:	kubernetes.default.svc.cluster.local
Address: 169.169.0.1
```

## 问题

### 1、dns内外网配置

```shell
默认的coredns yaml配置文件的dnsPolicy配置为Default
内网配置应为：dnsPolicy: ClusterFirst

DNSPolicy

一共有4种：

None
Default
ClusterFirst
ClusterFirstHostNet

1，None就是不载入任何预先设定的DNS配置，因此这种情况下，比必须配置DNSConfig，不然根本没有DNS可用；

2，Default是说DNS集成该Pod的Node的DNS设置；注意：该Default不是“DNSPolicy的缺省配置”！

3，ClusterFirst，和2完全相反，是说Pod的DNS配置使用kube-dns的预先配置，并且这个才是DNSPolicy的缺省配置。比如，对于一个没有配置DNSPolicy，也没有配置DNSConfig的Pod来说，当Pod启动后，其DNS配置如下所示：

4，ClusterFirstWithHostNet“: 和ClusterFirst一样，不过是Pod运行在hostNetwork:true的情况下强制指定的。
```

